{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1778d6-ff07-4706-a949-41a5c95f7c82",
   "metadata": {},
   "source": [
    "### First: Split the data in batchs for your given usage tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba62baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the raw data \n",
    "df = pd.read_csv('./your_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5377a20-17ae-474e-840e-cb5984b4bfd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "def create_jsonl(df, output_base_name, output_dir, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into multiple .jsonl files, each containing up to batch_size rows, \n",
    "    and stores the files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame containing the input data.\n",
    "    - output_base_name: Base name for the output files (e.g., 'output' will generate 'output_1.jsonl', 'output_2.jsonl', etc.).\n",
    "    - output_dir: Directory where the files will be stored.\n",
    "    - batch_size: Number of rows per .jsonl file (default is 20,000 rows).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate how many files are needed\n",
    "    total_rows = len(df)\n",
    "    total_files = math.ceil(total_rows / batch_size)\n",
    "\n",
    "    for i in range(total_files):\n",
    "        # Define the filename for the current batch\n",
    "        output_file = os.path.join(output_dir, f\"{output_base_name}_{i + 1}.jsonl\")\n",
    "\n",
    "        # Select the subset of rows for this batch\n",
    "        start_row = i * batch_size\n",
    "        end_row = min(start_row + batch_size, total_rows)\n",
    "        df_batch = df.iloc[start_row:end_row]\n",
    "\n",
    "        # Create the .jsonl file and write each row in JSON format\n",
    "        with open(output_file, 'w') as file:\n",
    "            for _, row in df_batch.iterrows():\n",
    "                # Extract the content of 'text' and the 'id_post'\n",
    "                prompt = row['text'] # variable to classify\n",
    "                custom_id = row['id_post']\n",
    "                \n",
    "                # Create the dictionary in the format\n",
    "                json_line = {\n",
    "                    \"custom_id\": f\"{custom_id}\",  \n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": \"...context here...\"},\n",
    "                            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                            ...Your prompt goes here. Please classify the following text:....   {prompt}\n",
    "                             \"\"\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"max_tokens\": 200\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Write the line to the file in JSON format\n",
    "                file.write(json.dumps(json_line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66033fdd-db9b-4389-96b0-b4dc838f4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_jsonl(df, batch_size=30000, output_base_name=\"input_batch\", output_dir=\"./input_batchs/\")\n",
    "\n",
    "# 50.000 maximun number of requests per batch\n",
    "# You might want to create a special folder for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff891b74-e7d4-43a2-b1d1-e0b88a153995",
   "metadata": {},
   "source": [
    "# Now we need to upload every .jsonl to the OpenAI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c951626-e67f-4eee-9174-0b0ba5001a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# load .env file to environment\n",
    "load_dotenv()\n",
    "\n",
    "# my key for this project\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') \n",
    "\n",
    "# start openAI client using the key \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4717c20-63d8-48d1-b830-4df300ac2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "\n",
    "# Function to upload all .jsonl files from a folder to OpenAI and save file IDs\n",
    "def upload_jsonl_files(folder_path, output_file=\"uploaded_file_ids.csv\"):\n",
    "    \"\"\"\n",
    "    Upload all .jsonl files from a folder to OpenAI, using the original filename for each upload.\n",
    "    The files are uploaded in numerical order.\n",
    "    After each successful upload, the file ID is saved to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - folder_path: Path to the folder containing the .jsonl files.\n",
    "    - output_file: The name of the CSV file where the file IDs will be saved (default: \"uploaded_file_ids.csv\").\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load existing file IDs from CSV if the file exists\n",
    "    if os.path.exists(output_file):\n",
    "        df_ids = pd.read_csv(output_file)\n",
    "    else:\n",
    "        df_ids = pd.DataFrame(columns=[\"filename\", \"file_id\"])\n",
    "\n",
    "    # Use regex to extract numbers from filenames and sort them numerically\n",
    "    def extract_number(filename):\n",
    "        match = re.search(r'(\\d+)', filename)\n",
    "        return int(match.group(1)) if match else float('inf')\n",
    "    \n",
    "    # List all files in the folder and filter .jsonl files, then sort them by number\n",
    "    jsonl_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith(\".jsonl\")],\n",
    "        key=extract_number\n",
    "    )\n",
    "\n",
    "    # Iterate over the sorted .jsonl files and upload them\n",
    "    for filename in jsonl_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if the file has already been uploaded (skip if found in the CSV)\n",
    "        if filename in df_ids[\"filename\"].values:\n",
    "            print(f\"{filename} has already been uploaded. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Upload the file to OpenAI using client.files.create()\n",
    "            with open(file_path, 'rb') as f:\n",
    "                batch_input_file = client.files.create(\n",
    "                    file=f,\n",
    "                    purpose=\"batch\"\n",
    "                )\n",
    "            \n",
    "            # Print the response to verify successful upload\n",
    "            print(f\"Uploaded {filename} - Response: {batch_input_file}\")\n",
    "\n",
    "            # Collect the file ID and append it to the DataFrame\n",
    "            file_id = batch_input_file.id\n",
    "            new_entry = pd.DataFrame({\"filename\": [filename], \"file_id\": [file_id]})\n",
    "            df_ids = pd.concat([df_ids, new_entry], ignore_index=True)\n",
    "\n",
    "            # Save the updated DataFrame to the CSV file in the current working directory\n",
    "            df_ids.to_csv(output_file, index=False)\n",
    "            print(f\"File ID saved to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error message and continue with the next file\n",
    "            print(f\"Error uploading {filename}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_jsonl_files(\"./input_batchs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978da8c5-de97-45dd-b792-ae658db756e4",
   "metadata": {},
   "source": [
    "# Now it is time to start the batch job for every jsonl file uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3a31-be03-4172-9ba0-eca098038b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batch jobs for files using their file_id from a CSV file\n",
    "def create_batch_jobs(csv_file=\"uploaded_file_ids.csv\", start_file=1, end_file=None, batch_ids_file=\"batch_ids.csv\"):\n",
    "    \"\"\"\n",
    "    Create batch jobs for files listed in a CSV file (with filename and file_id).\n",
    "    Iterates over a range of files (from start_file to end_file) and creates batch jobs for them.\n",
    "    Saves the batch ID and filename to a CSV file (batch_ids_file) after each successful batch creation.\n",
    "\n",
    "    Args:\n",
    "    - csv_file: The CSV file containing 'filename' and 'file_id' (default: \"uploaded_file_ids.csv\").\n",
    "    - start_file: The starting file number (default: 1).\n",
    "    - end_file: The ending file number (default: None, meaning all files).\n",
    "    - batch_ids_file: The CSV file to save 'batch_id' and 'filename' (default: \"batch_ids.csv\").\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the CSV with file IDs and filenames\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Determine the end_file if not provided (default to all files)\n",
    "    if end_file is None:\n",
    "        end_file = len(df)\n",
    "\n",
    "    # Load existing batch IDs if the CSV already exists, else create an empty DataFrame\n",
    "    if os.path.exists(batch_ids_file):\n",
    "        df_batches = pd.read_csv(batch_ids_file)\n",
    "    else:\n",
    "        df_batches = pd.DataFrame(columns=[\"filename\", \"batch_id\"])\n",
    "\n",
    "    # Adjust for zero-based indexing in the DataFrame (start_file - 1)\n",
    "    for idx, row in df.iloc[start_file - 1:end_file].iterrows():\n",
    "        filename = row['filename']\n",
    "        file_id = row['file_id']\n",
    "\n",
    "        # Generate the batch name based on the filename\n",
    "        batch_name = f\"batch_{filename}\"\n",
    "        \n",
    "        try:\n",
    "            # Create the batch job using client.batches.create()\n",
    "            batch = client.batches.create(\n",
    "                input_file_id=file_id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\",\n",
    "                metadata={\n",
    "                    \"description\": filename  # Use the filename in the description\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Print the response to verify successful batch creation\n",
    "            print(f\"Batch {batch_name} created with description: {filename} - Response: {batch}\")\n",
    "\n",
    "            # Extract the batch_id from the response and append to the DataFrame\n",
    "            batch_id = batch.id\n",
    "            new_entry = pd.DataFrame({\"filename\": [filename], \"batch_id\": [batch_id]})\n",
    "            df_batches = pd.concat([df_batches, new_entry], ignore_index=True)\n",
    "\n",
    "            # Save the updated DataFrame to the CSV file\n",
    "            df_batches.to_csv(batch_ids_file, index=False)\n",
    "            print(f\"Batch ID for {filename} saved to {batch_ids_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error message and continue with the next batch\n",
    "            print(f\"Error creating batch for {filename}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batch_jobs(\"uploaded_file_ids.csv\", start_file=1, end_file=15)\n",
    "\n",
    "#For this task you need to know how many tokens you can process at the same time, depending on your usage tier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb701e-2260-4030-989c-90359e1e6f49",
   "metadata": {},
   "source": [
    "# Get the ID of each output file to download it sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b99821-ae96-4945-bacc-b75e82b2743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve batch information and update the CSV with output_file_id\n",
    "def update_batch_output_file_id(csv_file=\"batch_ids.csv\"):\n",
    "    \"\"\"\n",
    "    Retrieve batch information for each batch_id in the CSV and update the CSV with the output_file_id.\n",
    "    If output_file_id already exists for a row, the row is skipped.\n",
    "\n",
    "    Args:\n",
    "    - csv_file: The CSV file containing 'filename', 'batch_id', and optionally 'output_file_id'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file with batch IDs\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Check if the 'output_file_id' column exists, if not, create it\n",
    "    if 'output_file_id' not in df.columns:\n",
    "        df['output_file_id'] = None\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        batch_id = row['batch_id']\n",
    "        filename = row['filename']\n",
    "        output_file_id = row['output_file_id']\n",
    "\n",
    "        # Skip the row if the output_file_id is already populated\n",
    "        if pd.notna(output_file_id):\n",
    "            print(f\"Output file ID for {filename} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Retrieve the batch information from OpenAI API\n",
    "            batch_info = client.batches.retrieve(batch_id)\n",
    "\n",
    "            # Extract the output_file_id\n",
    "            new_output_file_id = batch_info.output_file_id\n",
    "\n",
    "            # Check if the output_file_id is not empty\n",
    "            if new_output_file_id:\n",
    "                # Update the DataFrame with the new output_file_id\n",
    "                df.at[idx, 'output_file_id'] = new_output_file_id\n",
    "\n",
    "                # Save the updated DataFrame to the CSV file\n",
    "                df.to_csv(csv_file, index=False)\n",
    "                print(f\"Output file ID for {filename} saved to {csv_file}\")\n",
    "            else:\n",
    "                print(f\"Output file ID for {filename} is empty. Skipping.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error message and continue with the next batch\n",
    "            print(f\"Error retrieving batch {batch_id}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_batch_output_file_id(\"batch_ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ec536-94c4-442a-acb9-85d75afcc227",
   "metadata": {},
   "source": [
    "# Dowloading every resulting .jsonl file after the batch jobs finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d12055-5683-4de7-a88b-fc84d229bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to download multiple files from OpenAI API based on a CSV file and save them\n",
    "def download_and_process_multiple_files(csv_file, download_folder):\n",
    "    \"\"\"\n",
    "    Download multiple files from OpenAI API based on the IDs in a CSV file, save them as .jsonl files,\n",
    "    and use the 'filename' column to name the saved files.\n",
    "\n",
    "    Args:\n",
    "    - csv_file: The CSV file containing 'output_file_id' and 'filename'.\n",
    "    - download_folder: The folder where the .jsonl files will be saved.\n",
    "    \"\"\"\n",
    "    # Create the download folder if it doesn't exist\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        output_file_id = row['output_file_id']\n",
    "        filename = row['filename']\n",
    "\n",
    "        # Skip rows where output_file_id is missing or empty\n",
    "        if pd.isna(output_file_id) or output_file_id == \"\":\n",
    "            print(f\"Output file ID for {filename} is missing. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Define the save path for the .jsonl file\n",
    "            save_path = os.path.join(download_folder, filename)\n",
    "\n",
    "            # Download the content from OpenAI API using the output_file_id\n",
    "            file_response = client.files.content(output_file_id)\n",
    "            \n",
    "            # Save the retrieved content to a .jsonl file\n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(file_response.text)\n",
    "            print(f\"File {filename} downloaded and saved to {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Print error message if something goes wrong\n",
    "            print(f\"Error downloading file {filename} with ID {output_file_id}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = \"batch_ids.csv\"  # The CSV file containing 'filename' and 'output_file_id'\n",
    "download_folder = \"./resulting_batches/\"  # The folder where files will be saved\n",
    "\n",
    "# You might want to create a special folder for this task\n",
    "\n",
    "# Download and process all files in the CSV\n",
    "download_and_process_multiple_files(csv_file, download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65af94-0c3c-415c-94f0-9a8921141f59",
   "metadata": {},
   "source": [
    "# Colleting the relevant information from every output .jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0c5eb-2780-4156-820c-00a8c39b2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl_to_dataframe(folder_path, id_column, content_column):\n",
    "    \"\"\"\n",
    "    Loads all .jsonl files from a specified folder, extracts relevant data (custom_id and assistant content),\n",
    "    and stores them in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path: Path to the folder containing the .jsonl files.\n",
    "    - id_column: The name of the column to store the 'custom_id' values.\n",
    "    - content_column: The name of the column to store the 'content' from the assistant role.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with the specified columns filled with data from the .jsonl files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to hold the extracted data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over all .jsonl files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Open and read the .jsonl file\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    # Parse the JSON line\n",
    "                    record = json.loads(line)\n",
    "                    \n",
    "                    # Extract the 'custom_id'\n",
    "                    custom_id = record.get('custom_id')\n",
    "                    \n",
    "                    # Extract the 'content' where the role is 'assistant'\n",
    "                    content = None\n",
    "                    response_body = record.get('response', {}).get('body', {})\n",
    "                    choices = response_body.get('choices', [])\n",
    "                    if choices:\n",
    "                        # Check if the message role is 'assistant'\n",
    "                        message = choices[0].get('message', {})\n",
    "                        if message.get('role') == 'assistant':\n",
    "                            content = message.get('content')\n",
    "                    \n",
    "                    # Append to data if both custom_id and content are found\n",
    "                    if custom_id and content:\n",
    "                        data.append({id_column: custom_id, content_column: content})\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    output_df = pd.DataFrame(data)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ba5a4-bace-4685-8b8a-3e4a11345b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = load_jsonl_to_dataframe(\"./resulting_batches/\", id_column=\"post_id\", content_column=\"reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8790f8-d643-49f1-9902-cbb3fd315f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"./complete_classification.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
